{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import goPhishing\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function #enforces python 3 syntax\n",
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from urllib.parse import urlparse\n",
    "import urllib\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import whois\n",
    "import regex\n",
    "import re\n",
    "import xml.etree.ElementTree\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "from io import StringIO as io\n",
    "import sys\n",
    "import dryscrape\n",
    "from random import shuffle\n",
    "import tldextract\n",
    "import socket\n",
    "socket.setdefaulttimeout(10) # or whatever timeout you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_uri = urlparse(url)\n",
    "print(parsed_uri)\n",
    "# Find domain from URL to search WHOIS\n",
    "domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "print(domainURL)\n",
    "# WHOIS query to pull name.\n",
    "#c = whois.query(domainURL)\n",
    "#domain = c.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipwhois\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(url)\n",
    "parsed_uri = urlparse(url)\n",
    "domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "print(domainURL)\n",
    "ip = socket.gethostbyname(domainURL)\n",
    "print(ip)\n",
    "domain = socket.gethostbyaddr(ip)[0]\n",
    "print(domain)\n",
    "ipwhodis = ipwhois.IPWhois(ip)\n",
    "results = ipwhodis.lookup_whois()\n",
    "\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ipwhodis.lookup_whois(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(results)\n",
    "\n",
    "for item in results['nets']:\n",
    "    print(item['updated'])\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = whois.query(self.split(\"//\")[-1].split(\"/\")[0].split('?')[0])\n",
    "# createDate = url.creation_date\n",
    "# print(createDate)\n",
    "\n",
    "parsed_uri = urlparse(url)\n",
    "# grab domain\n",
    "domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "ip = socket.gethostbyname(domainURL)\n",
    "# domain = socket.gethostbyaddr(ip)[0]\n",
    "# print(domain)\n",
    "\n",
    "ipwhodis = ipwhois.IPWhois(ip)\n",
    "results = ipwhodis.lookup_whois()\n",
    "for item in results['nets']:\n",
    "    createdDate = item['created']\n",
    "createDate = datetime.strptime(createdDate, '%Y-%m-%d')\n",
    "currentDate = datetime.now()\n",
    "dateDiff = currentDate-createDate\n",
    "dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "print(\"diff in years: \",dateDiffInYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.zlookup.com/'\n",
    "parsed_uri = urlparse(url)\n",
    "print(parsed_uri)\n",
    "domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "print(domainURL)\n",
    "ip = socket.gethostbyname(domainURL)\n",
    "print(ip)\n",
    "ipwhodis = ipwhois.IPWhois(ip)\n",
    "results = ipwhodis.lookup_rdap()\n",
    "# for key, values in results.items():\n",
    "#     print(key, values)\n",
    "\n",
    "# Open cmd to run whois query.\n",
    "# command = 'whois ' + domainURL\n",
    "#print(command)\n",
    "# resultant = str(os.popen(command).read())\n",
    "# print(resultant)\n",
    "#print(results)\n",
    "try:\n",
    "    domain = socket.gethostbyaddr(ip)\n",
    "except:\n",
    "    print('domain is invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalUrl(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Create header rows from dataset that was processed by goPhishing.py.\n",
    "\n",
    "newStr = ['havingIPAddress','urlLength','havingAtSymbol','doubleSlashRedirecting','prefixSuffix'\n",
    "          ,'havingSubDomain','sslFinalState','domainRegistrationLength','favicon','port'\n",
    "          ,'httpsToken','requestURL','urlOfAnchor','linksInTags','sfh','submittingToEmail',\n",
    "          'abnormalURL','redirect','onMouseOver','rightClick',\n",
    "          'popUpWindow','iFrame','ageOfDomain','dnsRecord','webTraffic',\n",
    "          ,'linksPointingToPage','statisticalReport']\n",
    "\n",
    "import csv\n",
    "with open('results_alexa.csv',newline='') as f:\n",
    "    r = csv.reader(f)\n",
    "    data = [line for line in r]\n",
    "with open('results_alexa.csv','w',newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(newStr)\n",
    "    w.writerows(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import section'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Later used for labeling tree data output. \n",
    "\n",
    "uci = pd.read_csv('results_openfish.csv')\n",
    "\n",
    "uci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.genfromtxt('results_openfish.csv',delimiter=',',dtype=np.int32)\n",
    "\n",
    "print(uci.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into x(features) and y(label)\n",
    "inputs = training_data[:,:-1]\n",
    "outputs = training_data[:,-1]\n",
    "\n",
    "\n",
    "#split data by 20%\n",
    "x_train ,x_test,y_train, y_test = train_test_split(inputs,outputs,test_size=0.3)       #test_size=0.2(whole_data)\n",
    "\n",
    "print(len(x_test))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_test = LogisticRegression()\n",
    "\n",
    "lgr_test.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lgr_test.predict(x_test)\n",
    "\n",
    "accuracy = 100.0 * accuracy_score(y_test, predictions)\n",
    "\n",
    "print (\"The accuracy of your Logistic Regression on testing data is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ats\n",
    "from datetime import datetime\n",
    "\n",
    "ak= 'AKIAIJJJWOX73BM2QTTQ'\n",
    "sk= 'd0T6LBAUsqFevPqN2Fz8BXu4MmXtElxvHue5o1au'\n",
    "time_stamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "date_stamp = datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "\n",
    "ats.AlexaTopSites(ak,sk).get_sites('1','10000','US',time_stamp, date_stamp)\n",
    "\n",
    "\n",
    "# with open ('top_alexa.json') as a:\n",
    "#     text = a.read().splitlines()\n",
    "#     for line in text:\n",
    "#         if \n",
    "   \n",
    "\n",
    "\n",
    "#ats.main()\n",
    "# -key ak, -secret sk, -country US, -count 100\n",
    "#os.system('python3 ats.py')\n",
    "print(type(topsites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "listing = []\n",
    "with open('top_alexa.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    for v in data.values():\n",
    "        url = 'http://www.' + str(v)\n",
    "        listing.append(url)\n",
    "        \n",
    "with open('top_sites.txt', 'w+') as f:\n",
    "    for item in listing:\n",
    "        f.write(item + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "testURLs = []\n",
    "with open('openfish.txt') as f:\n",
    "    for item in f:\n",
    "        \n",
    "        if counter < 25:\n",
    "            testURLs.append(item)\n",
    "            getageOfDomain(item)\n",
    "            counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for url in testURLs:\n",
    "    parsed_uri = urlparse(url)\n",
    "    #print('parsed:  ' + str(parsed_uri))\n",
    "    domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    #print('domainURL = '+ str(domainURL))\n",
    "    ip = socket.gethostbyname(domainURL)\n",
    "    #print(ip)\n",
    "    ipwhodis = ipwhois.IPWhois(ip)\n",
    "    results = ipwhodis.lookup_whois()\n",
    "    #print('results*** '+ str(results))\n",
    "    for item in results['nets']:\n",
    "        try:\n",
    "            createdDate = item['created']\n",
    "            match = re.search(r'\\d{4}-\\d{2}-\\d{2}', createdDate)\n",
    "            createDate = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "            #createDate = datetime.strftime(date, '%Y-%m-%d')\n",
    "            currentDate = datetime.now().date()\n",
    "            dateDiff = currentDate - createDate\n",
    "            dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "            print(dateDiffInYears)\n",
    "        except:\n",
    "            print('Found an error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retVal = 0\n",
    "for url in testURLs:\n",
    "    parsed_uri = urlparse(url)\n",
    "    domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    ip = socket.gethostbyname(domainURL)\n",
    "    ipwhodis = ipwhois.IPWhois(ip)\n",
    "    results = ipwhodis.lookup_whois()\n",
    "    for item in results['nets']:\n",
    "        createdDate = item['created']\n",
    "    \n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', createdDate)\n",
    "    date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "    \n",
    "    currentDate = datetime.now().date()\n",
    "    dateDiff = currentDate - date\n",
    "    dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "    # print(\"diff in years: \",dateDiffInYears)\n",
    "    if dateDiffInYears >= 10:\n",
    "        retVal = 1\n",
    "    else:\n",
    "        retVal = -1\n",
    "    \n",
    "print(retVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "testURLs = []\n",
    "with open('openfish.txt') as f:\n",
    "    for item in f:\n",
    "        \n",
    "        if counter < 25:\n",
    "            testURLs.append(item)\n",
    "            #abnormalUrl(item)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in testURLs:\n",
    "    parsed_uri = urlparse(url)\n",
    "    domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    ip = socket.gethostbyname(domainURL)\n",
    "    domain = socket.gethostbyaddr(ip)[0]\n",
    "    print(domainURL, ip, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in testURLs:\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "        parsed_html = BeautifulSoup(page, features='lxml') \n",
    "        try:\n",
    "            parseList = parsed_html.body.find('form').attrs\n",
    "            print(parseList)\n",
    "        except:\n",
    "            print('error in attrs')\n",
    "    except:\n",
    "        print('404 not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "testURLs = []\n",
    "with open('openfish.txt') as f:\n",
    "    for item in f:\n",
    "        \n",
    "        if counter < 15:\n",
    "            testURLs.append(item)\n",
    "            print(havingIP(item))\n",
    "            counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onMouseOver(self):\n",
    "    \"\"\"\n",
    "    This method looks for the on mouse over re-writing of links in the status bar.  \n",
    "    This type of ruse has become less effective as browsers usually ignore this.\n",
    "    \"\"\"\n",
    "    # This one is working however may be useless and outdated in usage. Will utilize until tested. \n",
    "    retVal = 0\n",
    "    try:\n",
    "        page = urllib.request.urlopen(self)\n",
    "        parsed_html = BeautifulSoup(page, features='lxml') \n",
    "        parseList = parsed_html.body.find('a').attrs\n",
    "        #print(parseList)\n",
    "        for key, values in parseList.items():\n",
    "            #print(key, values)\n",
    "            if key == 'onmouseover':\n",
    "                match = re.search(r'window.status',tag['onmouseover'])\n",
    "                if match:\n",
    "                    retVal = 1\n",
    "                else:\n",
    "                    retVal = -1\n",
    "            if key == 'href':  #matches the href=javascript tag\n",
    "                hrefMatch = re.search(r'javascript',tag['href'])\n",
    "                if hrefMatch:\n",
    "                    retVal = 1\n",
    "                else:\n",
    "                    retVal = -1\n",
    "    except:\n",
    "        print(\"exc\", \"onMouseOver\", \"On mouse over exception\")\n",
    "    #self.phishScore['onMouseOver'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "#testURLs = []\n",
    "with open('openfish.txt') as f:\n",
    "    for item in f:\n",
    "        \n",
    "        if counter < 5:\n",
    "            #testURLs.append(item)\n",
    "            print(onMouseOver(item))\n",
    "            counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in testURLs:\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "#         parsed_html = BeautifulSoup(page, features='lxml') \n",
    "#         parseList = parsed_html.body.find('a').attrs\n",
    "        \n",
    "    except:\n",
    "        print('No connection...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNonStandardPort(self):\n",
    "    '''May end up changing this just to check for port usage.'''\n",
    "    retVal = 0\n",
    "    try:\n",
    "        parsed_uri = urllib.parse.urlparse(self.url)\n",
    "        print(parsed_uri.port)\n",
    "        if (parsed_uri.port == None or  parsed_uri.port == 80 or parsed_uri.port == 443):\n",
    "            print('Parsed uri.port is: ' + str(parsed_uri.port))\n",
    "            retVal = -1\n",
    "        else:\n",
    "            retVal =1\n",
    "    except:\n",
    "        print(\"exc\", \"hasNonStandardPort\", \"Unknown Error\")\n",
    "\n",
    "    self.phishScore['port'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in testURLs:\n",
    "    print(urllib.parse.urlparse(url).port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_sites.txt') as f:\n",
    "    \n",
    "    for url in f:\n",
    "        print(url)\n",
    "        http = httplib2.Http()\n",
    "        status, response = http.request(url)\n",
    "        #print('{} is the url and the status is: {}'.format(url, status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newStr = ['havingIPAddress','urlLength','havingAtSymbol','doubleSlashRedirecting','prefixSuffix'\n",
    "          ,'havingSubDomain','sslFinalState','domainRegistrationLength','favicon','port'\n",
    "          ,'httpsToken','requestURL','urlOfAnchor','linksInTags','sfh','submittingToEmail'\n",
    "          ,'abnormalURL','redirect','onMouseOver','rightClick'\n",
    "          ,'popUpWindow','iFrame','ageOfDomain','dnsRecord','webTraffic'\n",
    "          ,'linksPointingToPage','statisticalReport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googlesearch import search\n",
    "# query = \"Geeksforgeeks\"\n",
    "  \n",
    "# for j in search(query, tld=\"co.in\", num=10, stop=1, pause=2): \n",
    "#     print(j)\n",
    "counter = 0\n",
    "testURLs = []\n",
    "with open('openfish.txt') as f:\n",
    "    for item in f:\n",
    "        \n",
    "        if counter < 15:\n",
    "            testURLs.append(item)#.strip('http://').strip('https://'))\n",
    "            #print(onMouseOver(item))\n",
    "            counter += 1\n",
    "# print(counter)\n",
    "# count = 0\n",
    "# for item in search(testURLs[4], tld='co.in', pause=6):\n",
    "#     print (item)\n",
    "#     count += 1\n",
    "# print(count)\n",
    "# print(testURLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "testURLsgood = []\n",
    "with open('openfish.txt') as f:\n",
    "    for item in f:\n",
    "        \n",
    "        if counter < 5:\n",
    "            testURLsgood.append(item.strip('\\n').strip('http://').strip('https://'))\n",
    "            #print(onMouseOver(item))\n",
    "            counter += 1\n",
    "print(testURLsgood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "http = httplib2.Http()\n",
    "status, response = http.request(url)\n",
    "soup = BeautifulSoup(response).prettify()\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexChecker(url):\n",
    "    '''This index checker is using a tool on the web that already works. Previous codes were deemed \n",
    "    inapprpriate by Google, and therefore violated user agreements. this method is slow as it uses selenium, \n",
    "    however it works and can help identify phishing sites. In this project, this will be part of the secondary\n",
    "    once a site was deemed phishing.'''\n",
    "    \n",
    "    retVal = 0\n",
    "    from selenium import webdriver, common\n",
    "    # Using Chrome to access web\n",
    "    driver = webdriver.Chrome()\n",
    "    # Open the website\n",
    "    driver.get('https://indexchecking.com/')\n",
    "    try:\n",
    "        find_urls = driver.find_element_by_name('f_urls')\n",
    "        find_urls.clear()\n",
    "        find_urls.send_keys(url)\n",
    "        button = driver.find_element_by_class_name('btn')\n",
    "        button.click()\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        if soup.body.findAll(text='Indexed'):\n",
    "            #print('ok')\n",
    "            retVal = 1\n",
    "        else:\n",
    "            #print('not ok')\n",
    "            retVal = -1\n",
    "    \n",
    "    except:\n",
    "        print(\"timedout....\")\n",
    "        retVal = 0\n",
    "        \n",
    "\n",
    "    driver.quit()\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in testURLsgood:\n",
    "    print('***Testing {}***'.format(item))\n",
    "    print(indexChecker(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "testURL = []\n",
    "counter = 0\n",
    "with open('openfish2.txt') as of:\n",
    "    for item in of:\n",
    "        testURL.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 15:\n",
    "    for url in testURL:\n",
    "        try:\n",
    "            http = httplib2.Http()\n",
    "            status, response = http.request(url)\n",
    "            sleep(3)\n",
    "            soup = BeautifulSoup(response, 'lxml')\n",
    "            print(soup.find_all('iframe'))\n",
    "            count += 1\n",
    "        except:\n",
    "            print('Error connecting')\n",
    "            count += 1\n",
    "#         try:\n",
    "#             r = requests.get(url)\n",
    "#             soup = BeautifulSoup(r, \"lxml\")\n",
    "#             print(soup.find_all('iframe'))\n",
    "#             sleep(3)\n",
    "#             count += 1\n",
    "#         except:\n",
    "#             print('Error connecting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "# count = 0\n",
    "# while count < 15:\n",
    "#     for item in testURL:\n",
    "#         try:\n",
    "#             html = urlopen(item)\n",
    "#         except HTTPError as e:\n",
    "#             print(e)\n",
    "#         except URLError as u:\n",
    "#             print(u)\n",
    "#         else:\n",
    "#             soup = BeautifulSoup(html.read(), features='lxml')\n",
    "#             soup.find_all('iframe')\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'http://data.phishtank.com/data/aefb2ca09d15cc83009f3c2892d8da303da24ad0008c516b501a30d092da9e77/online-valid.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# while count < 10:\n",
    "#     for item in cr:\n",
    "#         print(item)\n",
    "#         count += 1\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "while counter < 15:\n",
    "    for item in testURL:\n",
    "        if item in data['url']:\n",
    "            print(\"yes it is\")\n",
    "            counter += 1\n",
    "        else:\n",
    "            print('No it isnt')\n",
    "            counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data['url'][1])\n",
    "urltest = []\n",
    "for item in data['url']:\n",
    "    urltest.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openfish3.txt') as op1:\n",
    "    for item in op1:\n",
    "        if item in urltest:\n",
    "            print('It is!!!')\n",
    "    print('completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'https://www.globaltechbrasil.com/' in urltest:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = urlopen('https://openphish.com/feed.txt')\n",
    "openlist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = content.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data.decode()\n",
    "# print(newData)\n",
    "with open('test.txt','w+') as op:\n",
    "    op.write(newData)\n",
    "    for items in op:\n",
    "        print(type(items))\n",
    "        openlist.append(items)\n",
    "os.remove('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urltotest = \"http://www.cheapflightstoislamabad.co.uk/adm/index/home/login.php?cmd=login_submit&id=564005d438f66bbdc72dd10ef65a6ccb564005d438f66bbdc72dd10ef65a6ccb&session=564005d438f66bbdc72dd10ef65a6ccb564005d438f66bbdc72dd10ef65a6ccb\"    \n",
    "\n",
    "from selenium import webdriver, common\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iFrame(self, page_html):\n",
    "    retVal = 0\n",
    "    url = self.url\n",
    "    links = []\n",
    "    \n",
    "    answer = page_html.find_all('iframe')\n",
    "    if answer != []:\n",
    "        for item in page_html.find_all('iframe'):\n",
    "            link = item.get('src')\n",
    "            links.append(link)\n",
    "            for item in links:\n",
    "                if re.match(item, url):\n",
    "                    retVal = 1\n",
    "                else: \n",
    "                    retVal = -1\n",
    "    else:\n",
    "        retVal = 1\n",
    "\n",
    "\n",
    "    self.phishScore['iframe'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTML(self):\n",
    "    try:\n",
    "        chrome = webdriver.Chrome()\n",
    "        chrome.get(self)\n",
    "    except TimeoutException as timeout:\n",
    "        print(\"Connection timed out.\")\n",
    "    else:\n",
    "        page_html = BeautifulSoup(chrome.page_source, features='lxml')\n",
    "        chrome.quit()\n",
    "    return page_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = getHTML(urltotest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iFrame(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in html.find_all('iframe'):\n",
    "    print(link.get('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dum = 'dum'\n",
    "dummy = 'dummy'\n",
    "\n",
    "if re.match(dum, dummy):\n",
    "    print('yay')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phishFry2\n",
    "\n",
    "urltotest = \"http://www.cheapflightstoislamabad.co.uk/adm/index/home/login.php?cmd=login_submit&id=564005d438f66bbdc72dd10ef65a6ccb564005d438f66bbdc72dd10ef65a6ccb&session=564005d438f66bbdc72dd10ef65a6ccb564005d438f66bbdc72dd10ef65a6ccb\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Brites - MacBack/Phishfry/framework/phishFry2.py:92: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 92 of the file /Volumes/Brites - MacBack/Phishfry/framework/phishFry2.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(driver.page_source)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1, 'phishtank': 1, 'google': -1, 'openfish': 1, 'iframe': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 1, 'phishtank': 1, 'google': -1, 'openfish': 1, 'iframe': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishFry2.PhishyOrNo(urltotest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdx_toolkit\n",
    "\n",
    "cdx = cdx_toolkit.CDXFetcher(source='cc')\n",
    "url = 'commoncrawl.org/*'\n",
    "\n",
    "print(url, 'size estimate', cdx.get_size_estimate(url))\n",
    "\n",
    "for obj in cdx.iter(url, limit=1):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('samplecommoncrawl.txt', header=None,\n",
    "        names = ['timestamp','url','filename','mime','offset','digest','status','length','mimet'])\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlscc = []\n",
    "\n",
    "for item in data['url']:\n",
    "    item = item.strip(' \"url\": ')\n",
    "    urlscc.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in urlscc:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phishScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-daf7127db170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphishScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'phishScore' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://5000best.com/websites\"\n",
    "\n",
    "fivepages = getHTML(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sites = []\n",
    "for item in fivepages.find_all('a',{\"class\": \"n\"}):\n",
    "    site = item.get('href')\n",
    "    if re.match('/websites', site) == None:\n",
    "        sites.append(site)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://google.com', 'http://facebook.com', 'http://youtube.com', 'http://yahoo.com', 'http://twitter.com', 'http://amazon.com', 'http://ebay.com', 'http://cnn.com', 'http://linkedin.com', 'http://pinterest.com', 'http://nytimes.com', 'http://wikipedia.org', 'http://bing.com', 'http://apple.com', 'http://weather.com', 'http://msn.com', 'http://microsoft.com', 'http://wordpress.com', 'http://aol.com', 'http://tumblr.com', 'http://huffingtonpost.com', 'http://live.com', 'http://flickr.com', 'http://etsy.com', 'http://paypal.com', 'http://imdb.com', 'http://blogspot.com', 'http://ask.com', 'http://about.com', 'http://reddit.com', 'http://pandora.com', 'http://bbc.co.uk', 'http://nfl.com', 'http://walmart.com', 'http://usatoday.com', 'http://foxnews.com', 'http://dropbox.com', 'http://newegg.com', 'http://craigslist.org', 'http://swagbucks.com', 'http://nbcnews.com', 'http://hulu.com', 'http://cnet.com', 'http://wordpress.org', 'http://ehow.com', 'http://conduit.com', 'http://deviantart.com', 'http://vimeo.com', 'http://adobe.com', 'http://washingtonpost.com', 'http://target.com', 'http://nba.com', 'http://thepiratebay.se', 'http://bestbuy.com', 'http://stumbleupon.com', 'http://gizmodo.com', 'http://politico.com', 'http://stackoverflow.com', 'http://wired.com', 'http://soundcloud.com', 'http://mediafire.com', 'http://hootsuite.com', 'http://godaddy.com', 'http://netflix.com', 'http://blogger.com', 'http://instagram.com', 'http://ign.com', 'http://avg.com', 'http://groupon.com', 'http://cbssports.com', 'http://reuters.com', 'http://thefreedictionary.com', 'http://salesforce.com', 'http://go.com', 'http://southwest.com', 'http://indeed.com', 'http://woot.com', 'http://barackobama.com', 'http://drudgereport.com', 'http://lifehacker.com', 'http://answers.com', 'http://examiner.com', 'http://imgur.com', 'http://kickstarter.com', 'http://yelp.com', 'http://tripadvisor.com', 'http://latimes.com', 'http://allrecipes.com', 'http://cnbc.com', 'http://goodreads.com', 'http://cbsnews.com', 'http://zazzle.com', 'http://pogo.com', 'http://surveymonkey.com', 'http://bleacherreport.com', 'http://addthis.com', 'http://bloomberg.com', 'http://barnesandnoble.com', 'http://photobucket.com', 'http://att.com']\n"
     ]
    }
   ],
   "source": [
    "print(sites)\n",
    "\n",
    "with open('popularsites.txt', 'w+') as ps:\n",
    "    for item in sites:\n",
    "        ps.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankScoreCategoryAudienceURLLinksDescription1.9.1DiscoveryAllgoogle.comWIKI  A  Q  S  SIM Google - Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.2.8.8PeopleMF20facebook.comWIKI  A  Q  S  SIM Welcome to Facebook - Facebook helps you connect and share with the people in your life.3.8.7VideosAllyoutube.comWIKI  A  Q  S  SIM YouTube - Share your videos with friends, family, and the world4.8.6PortalsAllyahoo.comWIKI  A  Q  S  SIM Yahoo!5.8.6DiscussionMF20twitter.comWIKI  A  Q  S  SIM Twitter - Instantly connect to what's most important to you. Follow your friends, experts, favorite celebrities, and breaking news.6.8.5CommerceAllamazon.comWIKI  A  Q  S  SIM Amazon.com: Online Shopping for Electronics, Apparel, Computers, Books, DVDs & more - Online shopping from the earth's biggest selection of books, magazines, music, DVDs, videos, electronics, computers, software, apparel & accessories, shoes, jewelry, tools & hardware, housewares, f7.8.3CommerceAllebay.comWIKI  A  Q  S  SIM Electronics, Cars, Fashion, Collectibles, Coupons and More Online Shopping | eBay - Buy and sell electronics, cars, fashion apparel, collectibles, sporting goods, digital cameras, baby items, coupons, and everything else on eBay, the world's online marketplace8.8.3NewsAllcnn.comWIKI  A  Q  S  SIM CNN.com - Breaking News, U.S., World, Weather, Entertainment & Video News - CNN.com delivers the latest breaking news and information on the latest top stories, weather, business, entertainment, politics, and more. For in-depth coverage, CNN.com provides special reports, video, audio, photo gal9.8.2JobsAlllinkedin.comWIKI  A  Q  S  SIM World's Largest Professional Network | LinkedIn - 200 million+ members | Manage your professional identity. Build and engage with your professional network. Access knowledge, insights and opportunities.10.8.2PicturesF30pinterest.comWIKI  A  Q  S  SIM Pinterest / Home 11.8.2NewsMF45nytimes.comWIKI  A  Q  S  SIM The New York Times - Breaking News, World News & Multimedia - Find breaking news, multimedia, reviews & opinion on Washington, business, sports, movies, travel, books, jobs, education, real estate, cars & more.12.8.2EducationAllwikipedia.orgWIKI  A  Q  S  SIM Wikipedia - Wikipedia, the free encyclopedia that anyone can edit.13.8.1DiscoveryMF45bing.comWIKI  A  Q  S  SIM Bing - Bing is a search engine that brings together the best of search and people in your social networks to help you spend less time searching and more time doing.14.8.1TechnologyMF30apple.comWIKI  A  Q  S  SIM Apple - Apple designs and creates iPod and iTunes, Mac laptop and desktop computers, the OS X operating system, and the revolutionary iPhone and iPad.15.8.1NewsFweather.comWIKI  A  Q  S  SIM National and Local Weather Forecast, Hurricane, Radar and Report - The Weather Channel and weather.com provide a national and local weather forecast for cities, as well as weather radar, report and hurricane coverage.16.8.1PortalsAllmsn.comWIKI  A  Q  S  SIM MSN.com - MSN is Microsoft's portal, offering news, sports, money, games, videos, entertainment & celebrity gossip, weather, shopping and more great content, as well as Windows Live services such as Hotmail and Messenger.17.8.0ToolsMmicrosoft.comWIKI  A  Q  S  SIM Microsoft Home Page | Devices and Services   - At Microsoft our mission and values are to help people and businesses throughout the world realize their full potential.18.8.0ArticlesMF20wordpress.comWIKI  A  Q  S  SIM WordPress.com - Get a Free Blog Here - Start a WordPress blog or create a free website in minutes. Choose from over 200 free, customizable themes. Free support from awesome humans.19.8.0PortalsMF45aol.comWIKI  A  Q  S  SIM AOL.com - News, Sports, Weather, Entertainment, Local & Lifestyle - AOL offers today's news, sports, stock quotes, weather, movie reviews, TV trends and more. Get free email, AIM access, online radio, videos and horoscopes -- all on AOL.com!20.8.0PicturesF20tumblr.comWIKI  A  Q  S  SIM Tumblr                     - Follow the world's creators.21.7.9NewsFhuffingtonpost.comWIKI  A  Q  S  SIM Breaking News and Opinion on The Huffington Post - The destination for news, blogs and original content offering coverage of US politics, entertainment, style, world news, technology and comedy.22.7.9ServicesAlllive.comWIKI  A  Q  S  SIM Sign In - Outlook.com is free modern email service from Microsoft. Get a clean clutter-free inbox, easily connect with Office Web Apps and SkyDrive and use it with your Hotmail account.23.7.9PicturesAllflickr.comWIKI  A  Q  S  SIM Welcome to Flickr - Photo Sharing - Flickr is almost certainly the best online photo management and sharing application in the world. Show off your favorite photos and videos to the world, securely and privately show content to your friends and family, or blog the photos and videos you take with a 24.7.9CommerceF30etsy.comWIKI  A  Q  S  SIM Etsy - Your place to buy and sell all things handmade, vintage, and supplies - Buy and sell handmade or vintage items, art and supplies on Etsy, the world's most vibrant handmade marketplace. Share stories through millions of items from around the world.25.7.9CommerceMF45paypal.comWIKI  A  Q  S  SIM Paypal26.7.9MoviesMF20imdb.comWIKI  A  Q  S  SIM IMDb - Movies, TV and Celebrities - IMDb, the world's most popular and authoritative source for movie, TV and celebrity content.27.7.8ArticlesMF20blogspot.comWIKI  A  Q  S  SIM Blogger: Create your free Blog28.7.8DiscussionMF45ask.comWIKI  A  Q  S  SIM Ask.com - What's Your Question? - Ask.com is the #1 question answering service that delivers the best answers from the web and real people - all in one place.29.7.8ArticlesF20about.comWIKI  A  Q  S  SIM About.com: Do more. - About.com is a valuable resource for content that helps people to solve the large and small needs of everyday life.30.7.8DiscussionM20reddit.comWIKI  A  Q  S  SIM Reddit: the front page of the internet - reddit: the front page of the internet31.7.8MusicF20pandora.comWIKI  A  Q  S  SIM Pandora32.7.8NewsAllbbc.co.ukWIKI  A  Q  S  SIM BBC - Homepage - Breaking news, sport, TV, radio and a whole lot more. The BBC informs, educates and entertains - wherever you are, whatever your age.33.7.7SportMnfl.comWIKI  A  Q  S  SIM NFL.com - Official Site of the National Football League - The official source for NFL news, video highlights, fantasy football, game-day coverage, schedules, stats, scores and more.34.7.7CommerceF45walmart.comWIKI  A  Q  S  SIM Walmart.com: Save money. Live better. - Online shopping for the largest selection of electronics, home furnishings, video games, baby gear and more. Shop online and save money to live better, at Walmart.com. 35.7.7NewsMF45usatoday.comWIKI  A  Q  S  SIM USA TODAY: Latest World and US News  - USATODAY.com - The Nation's Newspaper provides you with up-to-date coverage of US and international news, weather, entertainment, finance, and more.36.7.7NewsMF45foxnews.comWIKI  A  Q  S  SIM Fox News - Breaking News Updates | Latest News Headlines | Photos & News Videos - Breaking News, Latest News and Current News from FOXNews.com. Breaking news and video. Latest Current News: U.S., World, Entertainment, Health, Business, Technology, Politics, Sports.37.7.7FilesAlldropbox.comWIKI  A  Q  S  SIM Dropbox - Simplify your life - Dropbox is a free service that lets you bring your photos, docs, and videos anywhere and share them easily. Never email yourself a file again!38.7.7CommerceM20newegg.comWIKI  A  Q  S  SIM Newegg.com - Computer Parts, Laptops, Electronics, and More! - Providing excellent product selection and resources of computer parts and hardware, along with a wide selection of electronics and more. With fast-shipping!  Once you know, you Newegg!39.7.6CommerceFcraigslist.orgWIKI  A  Q  S  SIM Craigslist > Cities - List of all international craigslist.org online classifieds sites40.7.6CommerceF30swagbucks.comWIKI  A  Q  S  SIM Earn Reward Points and Redeem Them For Free Stuff at Swagbucks.com - Earn rewards and free stuff by searching and shopping online, answering surveys, and more at Swagbucks.com, a customer loyalty rewards program. Be rewarded today.41.7.6NewsF45nbcnews.comWIKI  A  Q  S  SIM Breaking News & Top Stories - World News, US & Local | NBC News - Visit NBCNEWS.com for breaking news, original journalism and videos. Stay current with the latest world news, business headlines, health, sports & entertainment.42.7.6TVF20hulu.comWIKI  A  Q  S  SIM Watch TV. Watch Movies. | Online | Free | Hulu - Watch TV shows and movies free online. Stream episodes of The Office, Glee, Family Guy, SNL and many more hit shows.43.7.6TechnologyM20cnet.comWIKI  A  Q  S  SIM Product reviews and prices, software downloads, and tech news - CNET44.7.6ArticlesMwordpress.orgWIKI  A  Q  S  SIM WordPress › Blog Tool, Publishing Platform, and CMS45.7.6ArticlesMF20ehow.comWIKI  A  Q  S  SIM EHow | How to Videos, Articles & More - Discover the expert in you. | eHow.com - Learn how to do just about everything at eHow. Find expert advice along with How To videos and articles, including instructions on how to make, cook, grow, or do almost anything.46.7.6WebMF45conduit.comWIKI  A  Q  S  SIM Conduit - Increase User Engagement & Web Traffic   - Join over 260,000 websites, brands and developers. Enjoy increased user engagement & brand awareness. Start engaging users now!47.7.6PicturesMF30deviantart.comWIKI  A  Q  S  SIM DeviantART: where ART meets application! - Art - community of artists and those devoted to art. Digital art, skin art, themes, wallpaper art, traditional art, photography, poetry / prose. Art prints.48.7.6VideosAllvimeo.comWIKI  A  Q  S  SIM Vimeo, Your Videos Belong Here - Vimeo is the home for high-quality videos and the people who love them.49.7.6ToolsMF45adobe.comWIKI  A  Q  S  SIM Adobe - Adobe is changing the world through digital experiences.  We help our customers create, deliver, and optimize content and applications.50.7.6NewsMF45washingtonpost.comWIKI  A  Q  S  SIM Washington Post: Breaking News, World, US, DC News & Analysis - Breaking news and analysis on politics, business, world national news, entertainment more. In-depth DC, Virginia, Maryland news coverage including traffic, weather, crime, education, restaurant reviews and more.51.7.6CommerceF30target.comWIKI  A  Q  S  SIM Target.com : Furniture, Baby, Electronics, Toys - Expect more pay less with Target. Spend $50, get free shipping on over 500K items. Choose from a wide selection of furniture, baby, electronics, toys, shoes, c52.7.6SportM20nba.comWIKI  A  Q  S  SIM NBA.com - The official site of the National Basketball Association. Includes news, features, multimedia, player profiles, chat transcripts, schedules and statistics.53.7.6FilesM30thepiratebay.seWIKI  A  Q  S  SIM Download music, movies, games, software! The Pirate Bay - The galaxy's most resilient BitTorrent site - Download music, movies, games, software and much more. The Pirate Bay is the world's largest bittorrent tracker.54.7.6CommerceAllbestbuy.comWIKI  A  Q  S  SIM Best Buy: Making Technology Work for You - Shop Best Buy for electronics, computers, appliances, cell phones, video games & more. Choose free shipping or same day in-store pickup on thousands of items.55.7.6DiscoveryF20stumbleupon.comWIKI  A  Q  S  SIM Explore more. Web pages, photos, and videos | StumbleUpon.com - StumbleUpon is the easiest way to discover new and interesting web pages, photos and videos across the Web.56.7.6TechnologyM20gizmodo.comWIKI  A  Q  S  SIM Gizmodo, the Gadget Guide - Gizmodo is the go-to authority for gadget news and digital culture.57.7.6NewsM45politico.comWIKI  A  Q  S  SIM Politics, Political News - POLITICO.com - POLITICO covers political news with a focus on national politics, Congress, Capitol Hill, lobbying, advocacy, and more.  POLITICO's in-depth coverage includes video features, regular blogs, photo galleries, cartoons, and political forums.58.7.6DiscussionM30stackoverflow.comWIKI  A  Q  S  SIM Stack Overflow59.7.6TechnologyM20wired.comWIKI  A  Q  S  SIM Wired.com  - Get in-depth coverage of current and future trends in technology, and how they are shaping business, entertainment, communications, science, politics, and culture at Wired.com.60.7.6MusicM20soundcloud.comWIKI  A  Q  S  SIM SoundCloud - Hear the world’s sounds - Explore the largest community of artists, bands, podcasters and creators of music & audio61.7.6FilesMmediafire.comWIKI  A  Q  S  SIM Free Online Storage - MediaFire - MediaFire is the simplest free online storage service for businesses, professionals, and individuals to store, backup, collaborate and share files with others.62.7.5ToolsF30hootsuite.comWIKI  A  Q  S  SIM Social Media Management Dashboard - HootSuite - Enhance your social media management with HootSuite, the leading social media dashboard. Manage multiple networks and profiles and measure your campaign results.63.7.5WebMgodaddy.comWIKI  A  Q  S  SIM Domain Names | The World's Largest Domain Name Registrar - GoDaddy.com  - Go Daddy makes registering Domain Names fast, simple, and affordable. Find out why so many business owners chose Go Daddy to be their Domain Name Registrar.64.7.5MoviesMF20netflix.comWIKI  A  Q  S  SIM Netflix - Watch TV Shows Online, Watch Movies Online - Watch Movies & TV Shows Online or Streaming right to your TV via Xbox, Wii, PS3 & many other devices. Only $7.99/mo. Start Your Free Trial Today.65.7.5ArticlesF20blogger.comWIKI  A  Q  S  SIM Blogger: Create your free Blog66.7.5PicturesFinstagram.comWIKI  A  Q  S  SIM Instagram67.7.5GamesM30ign.comWIKI  A  Q  S  SIM Video Games, Wikis, Cheats, Walkthroughs, Reviews, News & Videos - IGN - IGN is your site for Xbox 360, PS3, Wii, PC, 3DS, PS Vita & iPhone games with expert reviews, news, previews, trailers, cheat codes, wiki guides & walkthroughs68.7.5ToolsMF45avg.comWIKI  A  Q  S  SIM AVG | Antivirus and Internet Security | Virus Protection - AVG Technologies (NYSE:AVG) is a global leader in security software, protecting more than 110 million consumers and small business computer users.69.7.5CommerceF30groupon.comWIKI  A  Q  S  SIM Groupon70.7.5SportMcbssports.comWIKI  A  Q  S  SIM Sports - CBSSports.com Sports News, Fantasy Scores, Sports Video - CBSSports.com features live scoring and news for NFL football, MLB baseball, NBA basketball, NHL hockey, college basketball and football. CBSSports.com is also your source for fantasy sports news71.7.5NewsM45reuters.comWIKI  A  Q  S  SIM Business & Financial News, Breaking US & International News | Reuters.com - Reuters.com brings you the latest news from around the world, covering breaking news in business, politics, entertainment, technology, and more in video and pictures.72.7.5EducationMF20thefreedictionary.comWIKI  A  Q  S  SIM Dictionary, Encyclopedia and Thesaurus - The Free Dictionary - Online Dictionary - Multiple dictionaries including: English dictionary, medical dictionary, legal dictionary, financial dictionary, computer dictionary, thesaurus, dictionary of acronyms and abbreviations, dictionary of idioms, thesaur73.7.5ServicesMF30salesforce.comWIKI  A  Q  S  SIM CRM and Cloud Computing To Grow Your Business - Salesforce.com - Customer relationship management (CRM) software & cloud computing from the leader in CRM solutions for businesses large & small. Free 30-day trial.74.7.5PortalsM20go.comWIKI  A  Q  S  SIM GO.com - Official Home Page - Discover how GO.com can launch your online experience with Search, fun stuff to do, and the latest Sports, News, Entertainment, and Movies from the top sources on the web. Make GO your home page today!75.7.5TravelF45southwest.comWIKI  A  Q  S  SIM Southwest Airlines | Book Flights, Airline Tickets, Airfare - Southwest has the best deals on flights, hotels and car rentals.76.7.5JobsF30indeed.comWIKI  A  Q  S  SIM Job Search | one search. all jobs. Indeed.com - Click here to find millions of jobs from thousands of company web sites, job boards and newspapers. one search. all jobs. Indeed.77.7.5CommerceMF30woot.comWIKI  A  Q  S  SIM Woot78.7.5NewsF45barackobama.comWIKI  A  Q  S  SIM Barack Obama - BarackObama.com is the official re-election campaign website of President Barack Obama. Visit the site for the latest updates from the Obama campaign, including news, videos, and information on how to volunteer and donate to the campaign.79.7.5NewsM45drudgereport.comWIKI  A  Q  S  SIM DRUDGE REPORT 2013®80.7.5TechnologyM20lifehacker.comWIKI  A  Q  S  SIM Lifehacker, tips and downloads for getting things done - Lifehacker curates tips, tricks, and technology for living better in the digital age.81.7.5DiscussionMF20answers.comWIKI  A  Q  S  SIM Answers - The Most Trusted Place for Answering Life's Questions82.7.5EntertainmentFexaminer.comWIKI  A  Q  S  SIM Welcome to Examiner.com | Examiner.com - Get the latest news and articles on topics that interest you - from our team of local insiders.83.7.5PicturesM30imgur.comWIKI  A  Q  S  SIM Imgur: the simple image sharer - Imgur is used to share photos with social networks and online communities, and has the funniest pictures from all over the Internet.84.7.5OrganizationsAllkickstarter.comWIKI  A  Q  S  SIM Kickstarter - Kickstarter is the world's largest funding platform for creative projects.85.7.5DiscussionF30yelp.comWIKI  A  Q  S  SIM San Francisco Restaurants, Dentists, Bars, Beauty Salons, Doctors - San Francisco   User Reviews and Recommendations of Top Restaurants, Shopping, Nightlife, Entertainment, Services and More at Yelp86.7.5TravelFtripadvisor.comWIKI  A  Q  S  SIM Reviews of Hotels, Flights and Vacation Rentals - TripAdvisor - TripAdvisor - Unbiased hotel reviews, photos and travel advice for hotels and vacations - Compare prices with just one click.87.7.5NewsMF45latimes.comWIKI  A  Q  S  SIM Los Angeles Times - California, national and world news - latimes.com - The Los Angeles Times is a leading source of news on Southern California, entertainment, movies, television, music, politics, business, health, technology, travel, sports, environment, economics, autos, jobs, real estate and ot88.7.5HealthFallrecipes.comWIKI  A  Q  S  SIM Allrecipes - Recipes and cooking confidence for home cooks everywhere   - Allrecipes is the #1 place for recipes, cooking tips, and how-to food videos—all rated and reviewed by millions of home cooks. Allrecipes makes it easy to find everyday recipes for chicken, make the perfect birthday cake, o89.7.4BusinessM45cnbc.comWIKI  A  Q  S  SIM Stock Market News, Business News, Financial, Earnings, World Markets - CNBC - Find Stock Market News, Business News, Financial, Earnings, World Markets.90.7.4BooksF20goodreads.comWIKI  A  Q  S  SIM Share Book Recommendations With Your Friends, Join Book Clubs, Answer Trivia - Discover and share books you love on Goodreads, the world's largest site for readers and book recommendations!91.7.4NewsMF45cbsnews.comWIKI  A  Q  S  SIM Breaking News Headlines: Business, Entertainment & World News - CBS News - News covering all the latest breaking national and world news headlines, including politics, sports, entertainment, business and more.92.7.4CommerceF20zazzle.comWIKI  A  Q  S  SIM Zazzle | Custom T-Shirts, Personalized Gifts, Posters, Art, and more   - Shop Millions of Art Products93.7.4GamesF45pogo.comWIKI  A  Q  S  SIM Play Free Online Games | Pogo.com® - Pogo is a great place to play free online games, including puzzle games, word games, and card games and the chance to Win Big Prizes!'/>94.7.4WebFsurveymonkey.comWIKI  A  Q  S  SIM SurveyMonkey: Free online survey software & questionnaire tool  -      Create and publish online surveys in minutes, and view results graphically and in real time. SurveyMonkey provides free online questionnaire and survey software. 95.7.4SportM20bleacherreport.comWIKI  A  Q  S  SIM Bleacher Report | Entertaining sports news, photos and slideshows - Sports journalists and bloggers covering NFL, MLB, NBA, NHL, MMA, college football and basketball, NASCAR, fantasy sports and more. News, photos, mock drafts, game scores, player profiles and more!96.7.4WebMF45addthis.comWIKI  A  Q  S  SIM The Largest Sharing and Social Data Platform. We Provide Twitter and Facebook Buttons, Custom Audience Targeting, and more. | AddThis - AddThis is the world's largest sharing platform. We use big data to help site owners and brands build digital audiences and use data to personalize experiences. We97.7.4BusinessM45bloomberg.comWIKI  A  Q  S  SIM Bloomberg - Business, Financial & Economic News, Stock Quotes - Bloomberg is a premier site for business and financial market news. It delivers world economic news, stock futures, stock quotes, & personal finance advice.98.7.4BooksF45barnesandnoble.comWIKI  A  Q  S  SIM Barnes & Noble - Books, Textbooks, eBooks, Toys, Games, DVDs and More - Lower Prices on Millions of Books, Movies and TV Show DVDs and Blu-ray, Music, Toys, and Games. Shop online for eBooks, NOOK, and textbooks. FREE Shipping on $25 orders!99.7.4PicturesMF20photobucket.comWIKI  A  Q  S  SIM Photo and image hosting, free photo galleries, photo editing | Photobucket - Get free image hosting, easy photo sharing, and photo editing. Upload pictures and videos, create with the online photo editor, or browse a photo gallery or album.100.7.4ServicesF45att.comWIKI  A  Q  S  SIM AT&T Cell Phones, U-verse, Digital TV, DSL Internet, and Phone Service  \t - AT&T is a leader in telecommunication services, including cell phones, wireless, U-verse, digital TV, high speed internet, DSL, home phone, and bundled services.\n",
      "Page 1 of 50   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] (5000 results)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for items in fivepages.findAll('div',{\"id\": \"content\"}):\n",
    "    print(items.text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSite():\n",
    "    count = 0\n",
    "    iterator = [\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"]\n",
    "    sites = []\n",
    "    while count < len(iterator):\n",
    "        iteration = iterator[count]\n",
    "        testurl = \"http://5000best.com/?w.c&xml=1&ta=32&p=2&sortby=0&ise=&h={}\".format(iteration)\n",
    "        findpages = getHTML(testurl)\n",
    "        \n",
    "        for item in findpages.find_all('a',{\"class\": \"n\"}):\n",
    "            site = item.get('href')\n",
    "            if re.match('/websites/', site) == None:\n",
    "                sites.append(site)\n",
    "        count += 1\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSites = buildSite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('popularsites.txt', 'w+') as ps:\n",
    "    for item in newSites:\n",
    "        ps.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
