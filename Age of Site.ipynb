{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function #enforces python 3 syntax\n",
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from urllib.parse import urlparse, urlsplit\n",
    "import urllib\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import whois\n",
    "\n",
    "import regex\n",
    "import re\n",
    "import xml.etree.ElementTree\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "#import StringIO as io\n",
    "from io import StringIO as io\n",
    "import sys\n",
    "import dryscrape\n",
    "from random import shuffle\n",
    "import tldextract\n",
    "import socket\n",
    "socket.setdefaulttimeout(10) # or whatever timeout you want\n",
    "\n",
    "\n",
    "url = input('Enter URL: ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(who.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ageofsite(x):\n",
    "#     create = x.creation_date\n",
    "#     now = datetime.now()\n",
    "#     age = now - create\n",
    "#     print(age)\n",
    "#     return\n",
    "\n",
    "# ageofsite(who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getageOfDomain(self):\n",
    "        \"\"\"Get age of domain. If it is less than 10 years old, it returns 0\"\"\"\n",
    "        # This one is working...\n",
    "        retVal =0\n",
    "        try:\n",
    "            url = whois.query(self.split(\"//\")[-1].split(\"/\")[0].split('?')[0])\n",
    "            createDate = url.creation_date\n",
    "            print(createDate)\n",
    "            currentDate = datetime.now()\n",
    "            dateDiff = currentDate-createDate\n",
    "            dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "            print(\"diff in years: \",dateDiffInYears)\n",
    "            if dateDiffInYears >= 10:\n",
    "                retVal =1\n",
    "            else:\n",
    "                retVal =-1\n",
    "        except:\n",
    "            print (\"exc\",\"getageOfDomain\" , \"Unknown Error\" )\n",
    "        #self.phishScore['getageOfDomain'] = retVal\n",
    "        return retVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getageOfDomain(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getAnchorResult(self):\n",
    "    \"\"\"Whether the domain of anchor is different from that of the website\"\"\"\n",
    "    # This one is working...\n",
    "    retVal =-1\n",
    "    try:\n",
    "        http = httplib2.Http()\n",
    "        status, response = http.request(self.url)\n",
    "        positiveAnchor =0\n",
    "        negativeAnchor =0\n",
    "        for link in BeautifulSoup(response, parseOnlyThese=SoupStrainer('a')):\n",
    "            if link.has_key('href'):\n",
    "                tldObj = tldextract.extract(link['href'])\n",
    "                if ( tldObj.domain == self.domainName and tldObj.suffix == self.domainSuffix ):\n",
    "                    positiveAnchor += 1\n",
    "                else:\n",
    "                    negativeAnchor += 1\n",
    "        ratio = negativeAnchor /(positiveAnchor + negativeAnchor)\n",
    "        if ratio > 0.5:  #site is considered Phishy\n",
    "            retVal =1\n",
    "        if ratio < 0.2:\n",
    "            retVal =-1\n",
    "    except:\n",
    "        print (\"exc\",\"getAnchorResult\",\"No Anchors were returned. Setting to Zero\")\n",
    "        pass\n",
    "    self.phishScore['urlOfAnchor'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAnchorResult(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def havingIP(self):\n",
    "#     # This one is working.\n",
    "#         retVal =-1\n",
    "#         try:\n",
    "#             parsed_uri = urlparse(url)\n",
    "#             domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "#             checkIP = re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\",self)\n",
    "#             if checkIP:\n",
    "#                 retVal =1\n",
    "#         except:\n",
    "#                 print(\"exc\" , \"havingIP\" , \"Unknown Error\")\n",
    "#         self.phishScore['havingIP'] = retVal\n",
    "#         return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "havingIP(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getGoogleIndex(self):\n",
    "# *********************THIS ONE WAS REMOVED**************************\n",
    "#         \"\"\"Adapted from:\n",
    "#         http://searchengineland.com/check-urls-indexed-google-using-python-259773\"\"\"\n",
    "#         try:\n",
    "#             #from bs4 import BeautifulSoup\n",
    "#             seconds = 5\n",
    "#             user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
    "#             headers = { 'User-Agent' : user_agent}\n",
    "#             retVal=1 # default to not indexed\n",
    "\n",
    "#             query = {'q': 'info:' + self.url}\n",
    "#             google = \"https://www.google.com/search?\" + urllib.urlencode(query)\n",
    "#             data = requests.get(google, headers=headers)\n",
    "#             data.encoding = 'ISO-8859-1'\n",
    "#             soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
    "\n",
    "#             check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\")\n",
    "#             href = check['href']\n",
    "#             retVal =-1\n",
    "#         except AttributeError:\n",
    "#             print(\"exc\",\"getGoogleIndex\" , \"Site is not indexed\")\n",
    "#         #self.phishScore['googleIndex'] = retVal\n",
    "#         return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlexaDelta(self):\n",
    "        # This one is now working.\n",
    "        import bs4 as bs\n",
    "        retVal =0\n",
    "        try:\n",
    "            http = httplib2.Http()\n",
    "            status, response = http.request(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+self)\n",
    "            delta = BeautifulSoup(response, 'xml').find(\"REACH\")['RANK']\n",
    "            #print(type(delta))\n",
    "            if int(delta) > 0:\n",
    "                retVal = 1\n",
    "            else:\n",
    "                retVal = -1\n",
    "        except:\n",
    "            #retVal =1   <---Unknown usage---removed to allow code to process properly and not give score if errored.\n",
    "            print(\"exc\",\"getAlexaDelta\" , \"Error happened\")\n",
    "        self.phishScore['pageRank'] = retVal\n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAlexaDelta(url)\n",
    "#print(response)\n",
    "# http = httplib2.Http()\n",
    "# status, response = http.request(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+url)\n",
    "# delta = BeautifulSoup(response, 'xml').find(\"REACH\")['RANK']\n",
    "\n",
    "# print(delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python\n",
    "# import urllib, sys, bs4\n",
    "# print (BeautifulSoup(http.request(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+ sys.argv[1]), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRedirect(self):\n",
    "    '''This one is now working. Identifies redirects.'''\n",
    "        retVal =0\n",
    "        try:\n",
    "            r = requests.get(self, allow_redirects=False)\n",
    "            if r.status_code==301:\n",
    "                retVal = 1\n",
    "            else:\n",
    "                retVal = -1\n",
    "        except:\n",
    "            if self.debugging:\n",
    "                print(\"exc\" , \"getRedirect\" ,\"Could not Contact {}\".format(self))\n",
    "\n",
    "        self.phishScore['redirect'] = retVal\n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRedirect(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def getLinksInTags(self):\n",
    "        \"\"\"Links in <Meta>, <Script> and <Link> tags  point at same domain\"\"\"\n",
    "        # This one is working. \n",
    "        retVal = 0\n",
    "\n",
    "        try:\n",
    "            http = httplib2.Http()\n",
    "            status, response = http.request(self)\n",
    "            metaTags = BeautifulSoup(response,'html.parser')\n",
    "            \n",
    "            matchedDomains =0\n",
    "            unMatchedDomains =0\n",
    "            for tag in metaTags:\n",
    "                \n",
    "                content =\"\"\n",
    "                if tag.has_attr('content'):\n",
    "                    content += (tag['content'])\n",
    "                if tag.has_attr('src'):\n",
    "                    content += (tag['src'])\n",
    "                if tag.has_attr('link'):\n",
    "                    content += (tag['link'])\n",
    "                matchObj = re.match(r'([^a-zA-Z\\d]+|http[s]?://)?([a-z0-9|-]+)\\.?([a-z0-9|-]+)\\.([a-z0-9|-]+)',content,re.M|re.I)\n",
    "                if matchObj:\n",
    "                    subdomain = matchObj.group(2)\n",
    "                    midDomain = matchObj.group(3)\n",
    "                    topDomain = matchObj.group(4)\n",
    "                    if domain.find(midDomain) != -1:  #we have a url that matches the domain of the site\n",
    "                        matchedDomains +=1\n",
    "                    else:\n",
    "                        unMatchedDomains +=1\n",
    "            print(\"Matched domains = {}\".format(matchedDomains))\n",
    "            print(\"unMatched domains = {}\".format(unMatchedDomains))\n",
    "            \n",
    "            percentUnmatched = unMatchedDomains/(matchedDomains+unMatchedDomains)\n",
    "\n",
    "            if percentUnmatched > 0.5:  #site is considered Phishy\n",
    "                retVal =1\n",
    "            else:\n",
    "                retVal =-1\n",
    "        except httplib2.ServerNotFoundError:\n",
    "                print(\"exc\",\"getLinksInTags\",\"Site is Down\")\n",
    "                pass\n",
    "        except:\n",
    "                print(\"exc\",\"getLinksInTags\",\"No tags were returned.  Setting to Zero\")\n",
    "                pass\n",
    "        self.phishScore['linksInTags'] = retVal\n",
    "        return retVal\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLinksInTags(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "http = httplib2.Http()\n",
    "status, response = http.request(url)\n",
    "#print(response)\n",
    "metaTags = BeautifulSoup(response,'html.parser', parse_only=SoupStrainer(['meta','script','link']))\n",
    "print(metaTags)\n",
    "matchedDomains =0\n",
    "unMatchedDomains =0\n",
    "for tag in metaTags:\n",
    "    print(tag)\n",
    "    content =\"\"\n",
    "    if tag.has_attr('content'):\n",
    "        content += (tag['content'])\n",
    "    if tag.has_attr('src'):\n",
    "        content += (tag['src'])\n",
    "    if tag.has_attr('link'):\n",
    "        content += (tag['link'])\n",
    "    matchObj = re.match(r'([^a-zA-Z\\d]+|http[s]?://)?([a-z0-9|-]+)\\.?([a-z0-9|-]+)\\.([a-z0-9|-]+)',content,re.M|re.I)\n",
    "    if matchObj:\n",
    "        subdomain = matchObj.group(2)\n",
    "        midDomain = matchObj.group(3)\n",
    "        topDomain = matchObj.group(4)\n",
    "        if domain.find(midDomain) != -1:  #we have a url that matches the domain of the site\n",
    "            matchedDomains +=1\n",
    "        else:\n",
    "            unMatchedDomains +=1\n",
    "print(\"Matched domains = {}\".format(matchedDomains))\n",
    "print(\"unMatched domains = {}\".format(unMatchedDomains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiateWhoisDoc(self):\n",
    "        \"\"\"Limited to 50000 requests\"\"\"\n",
    "        try:\n",
    "            parsed_uri = urlparse(self)\n",
    "            domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "            self.whoisDomainName = domain\n",
    "            \n",
    "            # Open cmd to run whois query.\n",
    "            command = 'whois ' + domain\n",
    "            #print(command)\n",
    "            results = str(os.popen(command).read())\n",
    "\n",
    "            # Save query to text file for parsing.         \n",
    "            savedResults = open('results.txt', 'w+')\n",
    "            savedResults.write(results)\n",
    "            savedResults.close()\n",
    "\n",
    "            \n",
    "        except:\n",
    "            print(\"exc\",\"initiateWhoisDoc\" , \"error in initiateWhoisDoc\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiateWhoisDoc(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# Strip domain from URL.\n",
    "parsed_uri = urlparse(url)\n",
    "domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "#self.whoisDomainName = domain\n",
    "\n",
    "# Open cmd to run whois query.\n",
    "command = 'whois ' + domain\n",
    "#print(command)\n",
    "results = str(os.popen(command).read())\n",
    "\n",
    "\n",
    "# Save query to text file for parsing.         \n",
    "savedResults = open('results.txt', 'w+')\n",
    "savedResults.write(results)\n",
    "savedResults.close()\n",
    "\n",
    "# Parse through file in order to find update date from whois. \n",
    "with open('results.txt') as f:\n",
    "    for line in f:\n",
    "        if 'changed' in line:\n",
    "            match = re.search(r'\\d{4}-\\d{2}-\\d{2}', line) # searches for the date inside the txt.\n",
    "            date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "            currentDate = datetime.now().strftime('%Y-%m-%d')\n",
    "            currentDate2 = datetime.strptime(currentDate,'%Y-%m-%d').date()\n",
    "            dateDiff = currentDate2 - date\n",
    "            dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "            \n",
    "            \n",
    "        \n",
    "# Parse through query to find information.\n",
    "# with open(\"results.txt\") as f:\n",
    "#     for line in f:\n",
    "#         if 'nserver:' in line:\n",
    "#             print(line)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiateWhoisDoc(self):\n",
    "    \"\"\"Not limited. No longer using whoisapi\"\"\"\n",
    "    # This one is now working. \n",
    "    try:\n",
    "        # Strip domain from URL.\n",
    "        parsed_uri = urlparse(url)\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        self.whoisDomainName = domain\n",
    "\n",
    "        # Open cmd to run whois query.\n",
    "        command = 'whois ' + domain\n",
    "        results = str(os.popen(command).read())\n",
    "\n",
    "\n",
    "        # Save query to text file for parsing.         \n",
    "        savedResults = open('results.txt', 'w+')\n",
    "        savedResults.write(results)\n",
    "        savedResults.close()\n",
    "        \n",
    "    except:\n",
    "        print(\"exc\",\"initiateWhoisDoc\" , \"error in initiateWhoisDoc\")\n",
    "\n",
    "def domainRegistrationLength(self):\n",
    "    # This one is now working. \n",
    "    retVal =0\n",
    "    try:\n",
    "        initiateWhoisDoc(self)\n",
    "        \n",
    "        # Parse through file in order to find update date from whois. \n",
    "        with open('results.txt') as f:\n",
    "            for line in f:\n",
    "                if 'changed' in line:\n",
    "                    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', line) # searches for the date inside the txt.\n",
    "                    date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "                    currentDate = datetime.now().strftime('%Y-%m-%d')\n",
    "                    currentDate2 = datetime.strptime(currentDate,'%Y-%m-%d').date()\n",
    "                    dateDiff = currentDate2 - date\n",
    "                    dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "        if dateDiffInYears <= 0.5:\n",
    "            retVal =1\n",
    "        else:\n",
    "            retVal =-1\n",
    "    except:\n",
    "        print(\"exc\",\"domainRegistrationLength\" , \"Error occured with domainRegistrationLength:{}\".format(self.url))\n",
    "\n",
    "    self.phishScore['domainRegistrationLength'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainRegistrationLength(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasAtSymbol(self):\n",
    "    # This one is working. \n",
    "    retVal = 0\n",
    "    try:\n",
    "        if '@' in self:\n",
    "            retVal =-1\n",
    "        else:\n",
    "            retVal =1\n",
    "    except:\n",
    "        print(\"exc\" , \"hasAtSymbol\" , \"Unknown Error\")\n",
    "    self.phishScore['havingAtSymbol'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasAtSymbol(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDoubleSlash(url):\n",
    "    retVal = 0\n",
    "    try:\n",
    "        if '//' in url:\n",
    "            retVal =1\n",
    "        else:\n",
    "            retVal =1\n",
    "    except:\n",
    "        print(\"exc\",\"hasDoubleSlash\" , \"Unknown Error\")\n",
    "    #self.phishScore['havingAtSymbol'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasDoubleSlash(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNonStandardPort(self):\n",
    "'''May end up changing this just to check for port usage.'''\n",
    "    retVal =0\n",
    "    try:\n",
    "        parsed_uri = urlparse(self.url)\n",
    "        if (parsed_uri.port == None or  parsed_uri.port == 80 or parsed_uri.port == 443):\n",
    "            print('Parsed uri.port is: ' + str(parsed_uri.port))\n",
    "            retVal =-1\n",
    "        else:\n",
    "            retVal =1\n",
    "    except:\n",
    "        printFormat(\"exc\",\"hasNonStandardPort\",\"Unknown Error\")\n",
    "\n",
    "    self.phishScore['port'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hasNonStandardPort(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorteningService(self):\n",
    "        \"\"\"\n",
    "        This is functionally the same as the redirect. \n",
    "        We could use the redirect method here.\n",
    "        Or we could just check through a list of shortening sites and mark it if we see\n",
    "        one of those.  For now.  Just returning 0.\n",
    "        \"\"\"\n",
    "        # This one works. \n",
    "        retVal =0\n",
    "        try:\n",
    "            r = requests.get(self.url, allow_redirects=False)\n",
    "            if r.status_code==301:\n",
    "                retVal = 1\n",
    "            else:\n",
    "                retVal = -1\n",
    "        except:\n",
    "                printFormat (\"exc\",\"shorteningService\",\"Could not Contact {}\".format(self.url))\n",
    "        self.phishScore['shorteningService'] = retVal\n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorteningService(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasPopUpWindow(self):\n",
    "    \n",
    "    # This one works.\n",
    "    retVal =0\n",
    "    try:\n",
    "        page = urllib.request.urlopen(self.url)\n",
    "        soup = BeautifulSoup(page)\n",
    "        data = soup.find('script')\n",
    "        for tag in soup.findAll('script'):\n",
    "            stringTag = str(tag)\n",
    "            if re.search(r'.*open\\(|alert\\(|confirm\\(|prompt\\(.*',stringTag):\n",
    "                # If a tag is found, then code breaks and assures a positive. \n",
    "                retVal=1\n",
    "                break\n",
    "            else:\n",
    "                retVal=-1\n",
    "    except:\n",
    "            print(\"exc\",\"hasPopUpWindow\",\"Pop up window exception\")        \n",
    "    self.phishScore['popUpWindow'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hasPopUpWindow('https://howchoo.com/g/zdvmogrlngz/python-regexes-findall-search-and-match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from urllib.request import urlopen\n",
    "#url = 'http://www.popuptest.com/popuptest1.html'\n",
    "print(url)\n",
    "page = urllib.request.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "if 'window.open' in soup.text:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('nopey whopey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(popups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(page)\n",
    "data = soup.find('script')\n",
    "for tag in soup.findAll('script'):\n",
    "    stringTag = str(tag)\n",
    "    newList = []\n",
    "#     bob = re.search(r'.*open\\(|alert\\(|confirm\\(|prompt\\(.*',stringTag)\n",
    "#     print(type(bob))\n",
    "    if re.search(r'.*open\\(|alert\\(|confirm\\(|prompt\\(.*',stringTag):\n",
    "        \n",
    "        retVal = 1\n",
    "        return retVal\n",
    "    else:\n",
    "        retVal = -1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasHttpsToken(url):\n",
    "    \"\"\"\n",
    "    This method looks for https in the url \n",
    "    http://https-www-paypal-it-webapps-mpp-home.soft-hair.com/\n",
    "    \"\"\"\n",
    "    # This one now works.\n",
    "    retVal =0\n",
    "    parsed_uri = urlparse(url)\n",
    "    #print(parsed_uri)\n",
    "    \n",
    "    if '{uri.scheme}'.format(uri=parsed_uri) == 'https':\n",
    "        retVal =1\n",
    "    else:\n",
    "        httpsDomain = 'https://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "        print(httpsDomain)\n",
    "        try:\n",
    "            http = httplib2.Http()\n",
    "            status, response = http.request(httpsDomain)\n",
    "            retVal =1\n",
    "        except:\n",
    "            print (\"exc\",\"hasHttpsToken\" , \"Unknown error\")\n",
    "            retVal =-1\n",
    "        retVal =-1\n",
    "    #self.phishScore['httpsToken'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasHttpsToken('https://docs.python.org/3/library/urllib.parse.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_uri = urlparse(url)\n",
    "httpsDomain = 'https://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "http = httplib2.Http()\n",
    "status, response = http.request(httpsDomain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serverFormHandler(self):\n",
    "    \"\"\"\n",
    "    Server Form\n",
    "    \"\"\"\n",
    "    # This one is nor working properly now. Was not pointing properly before equalling a false positive.\n",
    "    retVal =0\n",
    "    try:\n",
    "        page = urllib.request.urlopen(self.url)\n",
    "        parsed_html = BeautifulSoup(page) \n",
    "        parseList = parsed_html.body.find('form').attrs\n",
    "        for key, values in parseList.items():\n",
    "        # Check to see what 'action' tag is doing.\n",
    "            if values == \"\" or values == None or values == \"about:blank\":\n",
    "                retVal =1\n",
    "            else:\n",
    "                retVal =-1\n",
    "    except:\n",
    "            print (\"exc\",\"serverFormHandler\",\"SFH exception\")\n",
    "    self.phishScore['sfh'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverFormHandler(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page = urllib.request.urlopen(url)\n",
    "parsed_html = BeautifulSoup(page)\n",
    "#print(parsed_html.attrs)\n",
    "print (parsed_html.body.find('form').attrs)#, attrs={'action':}))\n",
    "parseList = parsed_html.body.find('form').attrs\n",
    "for key, values in parseList.items():\n",
    "    if values == 'portal.swisscom.ch.php':\n",
    "        print(key)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def onMouseOver(self):\n",
    "        \"\"\"\n",
    "        This method looks for the on mouse over re-writing of links in the status bar.  \n",
    "        This type of ruse has become less effective as browsers usually ignore this.\n",
    "        \"\"\"\n",
    "        # This one is working however may be useless in outdated in usage. Will utilize until tested. \n",
    "        retVal =0\n",
    "        try:\n",
    "            page = urllib.request.urlopen(self.url)\n",
    "            parsed_html = BeautifulSoup(page) \n",
    "            parseList = parsed_html.body.find('a').attrs\n",
    "            print(parseList)\n",
    "            for key, values in parseList.items():\n",
    "                print(key, values)\n",
    "                if key == 'onmouseover':\n",
    "                    match = re.search(r'window.status',tag['onmouseover'])\n",
    "                    if match:\n",
    "                        retVal =1\n",
    "                    else:\n",
    "                        retVal =-1\n",
    "                if key == 'href':  #matches the href=javascript tag\n",
    "                    hrefMatch = re.search(r'javascript',tag['href'])\n",
    "                    if hrefMatch:\n",
    "                        retVal =1\n",
    "                    else:\n",
    "                        retVal =-1\n",
    "        except:\n",
    "            print(\"exc\",\"onMouseOver\",\"On mouse over exception\")\n",
    "        self.phishScore['onMouseOver'] = retVal\n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onMouseOver(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def abnormalUrl(self):\n",
    "    '''Fixed to use WHOIS from within Python to match parsed data from actual URL'''\n",
    "    parsed_uri = urlparse(self.url)\n",
    "    # Find domain from URL to search WHOIS\n",
    "    domainURL = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    # WHOIS query to pull name.\n",
    "    c = whois.query(domainURL)\n",
    "    domain = c.name\n",
    "    retVal =0\n",
    "#     print(url)\n",
    "#     print(domain)\n",
    "#     print(re.search(domain,url))\n",
    "    try:\n",
    "        if not re.search(domain,url):\n",
    "            retVal =1\n",
    "        else:\n",
    "            retVal =-1\n",
    "    except:\n",
    "        printFormat (\"exc\",\"abnormalUrl\" , \"Unknown Error\")\n",
    "    self.phishScore['abnormalUrl'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormalUrl(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whois\n",
    "parsed_uri = urlparse(url)\n",
    "domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "c = whois.query(domain)\n",
    "print(c.name)\n",
    "print(c.last_updated)\n",
    "print(c.registrar)\n",
    "print(c.creation_date)\n",
    "print(c.name_servers)\n",
    "#print(parsed_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFavIcon(self):\n",
    "    \"\"\"Whether the domain use a favicon for website or not\"\"\"\n",
    "    \n",
    "    # This one is now working.\n",
    "    retVal =1\n",
    "    try:\n",
    "        http = httplib2.Http()\n",
    "        parsed_uri = urlparse(self.url)\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        status, response = http.request('http://' + domain + '/favicon.ico')\n",
    "        if status['status'] == '200':\n",
    "            retVal = -1\n",
    "        else:\n",
    "            retVal = 1\n",
    "        \n",
    "    except:\n",
    "            print (\"exc\",\"getFavIcon\",\"Error for finding favicon\")\n",
    "    self.phishScore['favicon'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFavIcon(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = httplib2.Http()\n",
    "parsed_uri = urlparse(url)\n",
    "domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "status, response = http.request('http://' + domain + '/favicon.ico')\n",
    "if status['status'] == '200':\n",
    "    print('yes')\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geturlLength(url):\n",
    "    \"\"\"Overall length of each URL\"\"\"\n",
    "    # This one was already working.\n",
    "    retVal = 0\n",
    "    try:\n",
    "        if len(url) >= 75 :\n",
    "            retVal = 1\n",
    "        elif len(url) >= 54  :\n",
    "            retVal =0\n",
    "        else:\n",
    "            retVal =-1\n",
    "    except:\n",
    "        print(\"exc\",\"geturlLength\",\"Unknown Error\")\n",
    "\n",
    "    #self.phishScore['urlLength'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geturlLength(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def includePrefixSuffix(url):\n",
    "    \"\"\"If URL incldes '-' character, it has Prefix or Suffix  \"\"\"\n",
    "    retVal = 0\n",
    "    try:\n",
    "        if (url.find('-') >=0 ) :\n",
    "            retVal = 1\n",
    "        else:\n",
    "            retVal =-1\n",
    "    except:\n",
    "        print (\"exc\",\"includePrefixSuffix\" , \"UnknownError\" )\n",
    "\n",
    "    #self.phishScore['prefixSuffix'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "includePrefixSuffix(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def usingIPAddress(self):\n",
    "        \"\"\" Checks for IP address in the URL which identifies bypassed security features.\"\"\"\n",
    "        # This one was completely redone and is working now.\n",
    "        try:\n",
    "            parsed_uri = urlparse(self.url)\n",
    "            domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "            print(domain)\n",
    "            import socket\n",
    "            ip = socket.gethostbyname(domain)\n",
    "            print(ip)\n",
    "            if domain == ip:\n",
    "                retVal = 1\n",
    "            else:\n",
    "                retVal =-1\n",
    "        except:\n",
    "            printFormat(\"usingIPAddress\" , \"Unknown Error\")\n",
    "        self.phishScore['usingIPAddress'] = retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usingIPAddress(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url2 = 'http://104.248.143.105/cibc/bank/index.php'\n",
    "parsed_uri = urlparse(url)\n",
    "domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "print(domain)\n",
    "import socket\n",
    "ip = socket.gethostbyname(domain)\n",
    "print(ip)\n",
    "if domain == ip:\n",
    "    print('no!')\n",
    "else:\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def usingSubDomains(self):\n",
    "        \"\"\" If URL includes more than 3 dots, it is phishing web-site (except www. ) \"\"\"\n",
    "        # This one is now working.\n",
    "        tempURL = self.url\n",
    "        try:\n",
    "            if tempURL.startswith('www.'):\n",
    "                tempURL = tempURL[4:]\n",
    "            retVal = 0\n",
    "            if (tempURL.count('.') > 3 ) :\n",
    "                retVal = 1\n",
    "            else:\n",
    "                retVal = -1\n",
    "        except:\n",
    "            printFormat(\"usingSubDomains\" , \"Unknown error\")\n",
    "        self.phishScore['havingSubDomain'] = retVal\n",
    "        return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usingSubDomains(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if url.startswith('www.'):\n",
    "    url = url[4:]\n",
    "    print(url)\n",
    "    if (url.count('.') > 3 ) :\n",
    "        print('Lots o dots')\n",
    "    else:\n",
    "        print('we good bruh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNSRecord(url):\n",
    "    \"\"\"If DNS record in Whois is empty, the website might be a phishing one\"\"\"\n",
    "    retVal = 0\n",
    "    counter = 0\n",
    "    try:\n",
    "        parsed_uri = urlparse(url)\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        #self.whoisDomainName = domain\n",
    "\n",
    "        # Open cmd to run whois query.\n",
    "        command = 'whois ' + domain\n",
    "        #print(command)\n",
    "        results = str(os.popen(command).read())\n",
    "        results = results.splitlines()\n",
    "        for line in results:\n",
    "            if 'nserver' in line:\n",
    "                counter += 1\n",
    "            if counter < 1:\n",
    "                retVal =1\n",
    "            else:\n",
    "                retVal =-1\n",
    "    \n",
    "    except:\n",
    "        print (\"exc\",\"DNSRecord\",\"Unknown error\")\n",
    "    #self.phishScore['dnsRecord'] = retVal\n",
    "    \n",
    "    return retVal, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"results.txt\") as b:\n",
    "#     counter = 0\n",
    "#     for line in b:\n",
    "#         if 'nserver' in line:\n",
    "#             counter += 1\n",
    "            \n",
    "        \n",
    "# print(counter)\n",
    "\n",
    "DNSRecord(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may be better than utilizing a saved text file and commiting to the information within.\n",
    "\n",
    "parsed_uri = urlparse(url)\n",
    "domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "self.whoisDomainName = domain\n",
    "\n",
    "# Open cmd to run whois query.\n",
    "command = 'whois ' + domain\n",
    "#print(command)\n",
    "results = str(os.popen(command).read())\n",
    "results = results.splitlines()\n",
    "for line in results:\n",
    "    print(line)\n",
    "    \n",
    "\n",
    "for line in results:\n",
    "    if 'changed' in line:\n",
    "        match = re.search(r'\\d{4}-\\d{2}-\\d{2}', line) # searches for the date inside the txt.\n",
    "        date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "        currentDate = datetime.now().strftime('%Y-%m-%d')\n",
    "        currentDate2 = datetime.strptime(currentDate,'%Y-%m-%d').date()\n",
    "        dateDiff = currentDate2 - date\n",
    "        dateDiffInYears = (dateDiff.days + dateDiff.seconds/86400)/365.2425\n",
    "        print(dateDiffInYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAlexaRank(self):\n",
    "    try:\n",
    "        retVal = 0\n",
    "        result = dict()\n",
    "        url = \"http://www.alexa.com/siteinfo/\" + self.url\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page)\n",
    "        for span in soup.find_all('span'):\n",
    "            if span.has_attr(\"class\"):\n",
    "                if \"globleRank\" in span[\"class\"]:\n",
    "                    for strong in span.find_all(\"strong\"):\n",
    "                        if strong.has_attr(\"class\"):\n",
    "                            if \"metrics-data\" in strong[\"class\"]:\n",
    "                                result['Global'] = strong.text.strip('\\n\\n').strip(' ')\n",
    "\n",
    "        for item, value in result.items():\n",
    "            if value != '-':\n",
    "                if int(value) < 10000:\n",
    "                    retVal = 1\n",
    "            \n",
    "            elif value == '-':\n",
    "                reVal = -1\n",
    "                \n",
    "    except:\n",
    "        retVal =1\n",
    "        printFormat (\"exc\",\"getAlexaRank\",\"Unknown error\")\n",
    "    self.phishScore['statisticalReport'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAlexaRank(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = httplib2.Http()\n",
    "status, response = http.request(\"https://www.alexa.com/siteinfo/\"+ url)\n",
    "#print(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+ url)\n",
    "#print('Status is: {}'.format(status))\n",
    "#print('Response is: {}'.format(response))\n",
    "rank = BeautifulSoup(response,'xml')\n",
    "print(rank.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ats\n",
    "\n",
    "ak= 'AKIAJJDLVU7CV7PE2ZZA'\n",
    "sk= 'A74CyYvwCxGbijmbdZK0dT4fFGW/PAbM0fi8L7Bc'\n",
    "time_stamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "date_stamp = datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "\n",
    "topsites = ats.AlexaTopSites(ak,sk).get_sites('0','10000','US',time_stamp, date_stamp)\n",
    "\n",
    "# with open ('top_alexa.json') as a:\n",
    "#     text = a.read().splitlines()\n",
    "#     for line in text:\n",
    "#         if \n",
    "   \n",
    "\n",
    "\n",
    "#ats.main()\n",
    "# -key ak, -secret sk, -country US, -count 100\n",
    "#os.system('python3 ats.py')\n",
    "#print(type(topsites))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rank(self):\n",
    "    retVal = 0\n",
    "    result = dict()\n",
    "    url = \"http://www.alexa.com/siteinfo/\" + self.url\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    for span in soup.find_all('span'):\n",
    "        if span.has_attr(\"class\"):\n",
    "            if \"globleRank\" in span[\"class\"]:\n",
    "                for strong in span.find_all(\"strong\"):\n",
    "                    if strong.has_attr(\"class\"):\n",
    "                        if \"metrics-data\" in strong[\"class\"]:\n",
    "                            result['Global'] = strong.text.strip('\\n\\n').strip(' ')\n",
    "\n",
    "    for item, value in result.items():\n",
    "        if value != '-':\n",
    "            if int(value) < 10000:\n",
    "                retVal = 1\n",
    "                print('not phishy...')\n",
    "        elif value == '-':\n",
    "            reVal = -1\n",
    "            print('This is phishy...')\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is phishy...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Global': '-'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rank(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RequestURL(url):\n",
    "    \"\"\"whether src link to a out-domain website or not\"\"\"\n",
    "    retVal = 0\n",
    "    try:\n",
    "        parsed_uri = urlparse(url)\n",
    "        domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        response = requests.get(url).text\n",
    "        metaTags = BeautifulSoup(response)\n",
    "        tags = metaTags.findAll(attrs={\"src\" : True})\n",
    "        matchedDomains =0\n",
    "        unMatchedDomains =0\n",
    "        for tag in tags:\n",
    "            matchObj = re.match(r'([^a-zA-Z\\d]+|http[s]?://)?([a-z0-9|-]+)\\.?([a-z0-9|-]+)\\.([a-z0-9|-]+)',tag['src'],re.M|re.I)\n",
    "            if matchObj:\n",
    "                subdomain = matchObj.group(2)\n",
    "                midDomain = matchObj.group(3)\n",
    "                topDomain = matchObj.group(4)\n",
    "                if domain.find(midDomain) != -1:  #we have a url that matches the domain of the site\n",
    "                    matchedDomains +=1\n",
    "                else:\n",
    "                    unMatchedDomains +=1\n",
    "        if unMatchedDomains + matchedDomains > 1:\n",
    "            percentUnmatched = unMatchedDomains/(matchedDomains+unMatchedDomains)\n",
    "            if percentUnmatched > 0.5:  #site is considered Phishy\n",
    "                retVal =1\n",
    "            else:\n",
    "                retVal =-1\n",
    "        else:\n",
    "            retVal = -1\n",
    "    except:\n",
    "            print (\"exc\",\"RequestURL\",\"No tags were returned.\")\n",
    "    #self.phishScore['requestURL'] = retVal\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exc RequestURL No tags were returned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RequestURL('www.google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'www.reddit.com'\n",
    "parsed_uri = urlparse(url1)\n",
    "domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "response = requests.get(url).text\n",
    "metaTags = BeautifulSoup(response)\n",
    "tags = metaTags.findAll(attrs={\"src\" : True})\n",
    "matchedDomains =0\n",
    "unMatchedDomains =0\n",
    "for tag in tags:\n",
    "    matchObj = re.match(r'([^a-zA-Z\\d]+|http[s]?://)?([a-z0-9|-]+)\\.?([a-z0-9|-]+)\\.([a-z0-9|-]+)',tag['src'],re.M|re.I)\n",
    "    if matchObj:\n",
    "        subdomain = matchObj.group(2)\n",
    "        midDomain = matchObj.group(3)\n",
    "        topDomain = matchObj.group(4)\n",
    "        if domain.find(midDomain) != -1:  #we have a url that matches the domain of the site\n",
    "            matchedDomains +=1\n",
    "        else:\n",
    "            unMatchedDomains +=1\n",
    "if unMatchedDomains + matchedDomains > 1:\n",
    "    percentUnmatched = unMatchedDomains/(matchedDomains+unMatchedDomains)\n",
    "    if percentUnmatched > 0.5:  #site is considered Phishy\n",
    "        retVal =1\n",
    "    else:\n",
    "        retVal =-1\n",
    "else:\n",
    "    retVal = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
